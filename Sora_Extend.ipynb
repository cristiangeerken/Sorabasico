{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXbUOo9fJ0+9HX6bMZ/JfC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshumer/sora-extend/blob/main/Sora_Extend.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sora 2 — AI‑Planned, Scene‑Exact Prompts with Continuity (Chained >12s)\n",
        "\n",
        "Built by [Matt Shumer](https://x.com/mattshumer_).\n",
        "\n",
        "Pipeline:\n",
        "1) Use an LLM (“GPT‑5 Thinking”) to plan N scene prompts from a base idea. The LLM is prompted to do this intelligently to enable continuity.\n",
        "2) Render each segment with Sora 2; for continuity, pass the prior segment’s **final frame** as `input_reference`.\n",
        "3) Concatenate segments into a single MP4."
      ],
      "metadata": {
        "id": "R_OryDkwDDu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1) Install & imports\n",
        "\n",
        "import sys, subprocess, importlib.util, shutil, os, textwrap, tempfile\n",
        "\n",
        "def pip_install(*pkgs):\n",
        "    # Install into the *current* kernel's interpreter\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", *pkgs])\n",
        "\n",
        "def ensure(spec_name, *pip_pkgs):\n",
        "    if importlib.util.find_spec(spec_name) is None:\n",
        "        pip_install(*pip_pkgs)\n",
        "    return importlib.util.find_spec(spec_name) is not None\n",
        "\n",
        "MOVIEPY_AVAILABLE = ensure(\"moviepy\", \"moviepy>=2.0.0\", \"imageio\", \"imageio-ffmpeg\")\n",
        "\n",
        "# Try to import MoviePy if now available\n",
        "if MOVIEPY_AVAILABLE:\n",
        "    from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "else:\n",
        "    # Fallback concat uses ffmpeg (from imageio-ffmpeg or system)\n",
        "    try:\n",
        "        import imageio_ffmpeg\n",
        "        FFMPEG_BIN = imageio_ffmpeg.get_ffmpeg_exe()\n",
        "    except Exception:\n",
        "        FFMPEG_BIN = shutil.which(\"ffmpeg\")\n",
        "\n",
        "    if not FFMPEG_BIN:\n",
        "        # Final attempt to get ffmpeg via pip\n",
        "        pip_install(\"imageio-ffmpeg\")\n",
        "        import imageio_ffmpeg\n",
        "        FFMPEG_BIN = imageio_ffmpeg.get_ffmpeg_exe()\n",
        "\n",
        "    if not FFMPEG_BIN:\n",
        "        raise RuntimeError(\n",
        "            \"FFmpeg not found and MoviePy unavailable. \"\n",
        "            \"Install ffmpeg on your system or allow pip installs.\"\n",
        "        )\n",
        "\n",
        "print(\"MoviePy available:\", MOVIEPY_AVAILABLE)\n",
        "\n",
        "!pip -q install --upgrade openai requests opencv-python-headless imageio[ffmpeg]\n",
        "\n",
        "import os, re, io, json, time, math, mimetypes\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import cv2\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "from IPython.display import Video as IPyVideo, display\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "EiRiFUsnR3A-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Config\n",
        "\n",
        "Fill in:\n",
        "- OPENAI_API_KEY\n",
        "- SECONDS_PER_SEGMENT (options: 4, 8, 12)\n",
        "- NUM_GENERATIONS (this is the total number of segments we will generate and concatenate. to get the total length, do `SECONDS_PER_SEGMENT * NUM_GENERATIONS`)"
      ],
      "metadata": {
        "id": "Hpt3gFMtDMzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"Your API Key\"  # for quick local testing only (avoid in shared notebooks)\n",
        "\n",
        "client = OpenAI()  # uses OPENAI_API_KEY\n",
        "\n",
        "# ---------- Planner (text model) ----------\n",
        "# If you have access to \"GPT-5 Thinking\", set it below. Otherwise, fallback to a strong reasoning model you have.\n",
        "PLANNER_MODEL = os.environ.get(\"PLANNER_MODEL\", \"gpt-5\")\n",
        "\n",
        "# ---------- Sora (video model) ----------\n",
        "SORA_MODEL = \"sora-2\"        # or \"sora-2-pro\"\n",
        "SIZE       = \"1280x720\"      # must stay constant across segments\n",
        "\n",
        "# ---------- Your project inputs ----------\n",
        "BASE_PROMPT          = \"Gameplay footage of a game releasing in 2027, a car driving through a futuristic city\"\n",
        "SECONDS_PER_SEGMENT  = 8\n",
        "NUM_GENERATIONS      = 2\n",
        "\n",
        "# Output directory\n",
        "OUT_DIR = Path(\"sora_ai_planned_chain\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Polling cadence\n",
        "POLL_INTERVAL_SEC = 2\n",
        "PRINT_PROGRESS_BAR = True\n",
        "\n",
        "# Low-level HTTP for Sora Video API calls\n",
        "API_BASE = \"https://api.openai.com/v1\"\n",
        "HEADERS_JSON = {\"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\", \"Content-Type\": \"application/json\"}\n",
        "HEADERS_AUTH = {\"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"}"
      ],
      "metadata": {
        "id": "NO-yBwL-DLjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) The planner system prompt\n",
        "\n",
        "We’ll ask the planner model to output a clean JSON object with one prompt per generation.\n",
        "The prompts contain context and the actual shot details, maximizing continuity.\n",
        "\n",
        "This isn't super optimized and was a first pass done by GPT. If people like this notebook, let me know on X, and I'll improve this!"
      ],
      "metadata": {
        "id": "qCtddkS2TwG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PLANNER_SYSTEM_INSTRUCTIONS = r\"\"\"\n",
        "You are a senior prompt director for Sora 2. Your job is to transform:\n",
        "- a Base prompt (broad idea),\n",
        "- a fixed generation length per segment (seconds),\n",
        "- and a total number of generations (N),\n",
        "\n",
        "into **N crystal-clear shot prompts** with **maximum continuity** across segments.\n",
        "\n",
        "Rules:\n",
        "1) Return **valid JSON** only. Structure:\n",
        "   {\n",
        "     \"segments\": [\n",
        "       {\n",
        "         \"title\": \"Generation 1\",\n",
        "         \"seconds\": 6,\n",
        "         \"prompt\": \"<prompt block to send into Sora>\"\n",
        "       },\n",
        "       ...\n",
        "     ]\n",
        "   }\n",
        "   - `seconds` MUST equal the given generation length for ALL segments.\n",
        "   - `prompt` should include a **Context** section for model guidance AND a **Prompt** line for the shot itself,\n",
        "     exactly like in the example below.\n",
        "2) Continuity:\n",
        "   - Segment 1 starts fresh from the BASE PROMPT.\n",
        "   - Segment k (k>1) must **begin exactly at the final frame** of segment k-1.\n",
        "   - Maintain consistent visual style, tone, lighting, and subject identity unless explicitly told to change.\n",
        "3) Safety & platform constraints:\n",
        "   - Do not depict real people (including public figures) or copyrighted characters.\n",
        "   - Avoid copyrighted music and avoid exact trademark/logos if policy disallows them; use brand-safe wording.\n",
        "   - Keep content suitable for general audiences.\n",
        "4) Output only JSON (no Markdown, no backticks).\n",
        "5) Keep the **Context** lines inside the prompt text (they're for the AI, not visible).\n",
        "6) Make the writing specific and cinematic; describe camera, lighting, motion, and subject focus succinctly.\n",
        "\n",
        "Below is an **EXAMPLE (verbatim)** of exactly how to structure prompts with context and continuity:\n",
        "\n",
        "Example:\n",
        "Base prompt: \"Intro video for the iPhone 19\"\n",
        "Generation length: 6 seconds each\n",
        "Total generations: 3\n",
        "\n",
        "Clearly defined prompts with maximum continuity and context:\n",
        "\n",
        "### Generation 1:\n",
        "\n",
        "<prompt>\n",
        "First shot introducing the new iPhone 19. Initially, the screen is completely dark. The phone, positioned vertically and facing directly forward, emerges slowly and dramatically out of darkness, gradually illuminated from the center of the screen outward, showcasing a vibrant, colorful, dynamic wallpaper on its edge-to-edge glass display. The style is futuristic, sleek, and premium, appropriate for an official Apple product reveal.\n",
        "<prompt>\n",
        "\n",
        "---\n",
        "\n",
        "### Generation 2:\n",
        "\n",
        "<prompt>\n",
        "Context (not visible in video, only for AI guidance):\n",
        "\n",
        "* You are creating the second part of an official intro video for Apple's new iPhone 19.\n",
        "* The previous 6-second scene ended with the phone facing directly forward, clearly displaying its vibrant front screen and colorful wallpaper.\n",
        "\n",
        "Prompt: Second shot begins exactly from the final frame of the previous scene, showing the front of the iPhone 19 with its vibrant, colorful display clearly visible. Now, smoothly rotate the phone horizontally, turning it from the front to reveal the back side. Focus specifically on the advanced triple-lens camera module, clearly highlighting its premium materials, reflective metallic surfaces, and detailed lenses. Maintain consistent dramatic lighting, sleek visual style, and luxurious feel matching the official Apple product introduction theme.\n",
        "</prompt>\n",
        "\n",
        "---\n",
        "\n",
        "### Generation 3:\n",
        "\n",
        "<prompt>\n",
        "Context (not visible in video, only for AI guidance):\n",
        "\n",
        "* You are creating the third and final part of an official intro video for Apple's new iPhone 19.\n",
        "* The previous 6-second scene ended clearly showing the back of the iPhone 19, focusing specifically on its advanced triple-lens camera module.\n",
        "\n",
        "Prompt: Final shot begins exactly from the final frame of the previous scene, clearly displaying the back side of the iPhone 19, with special emphasis on the triple-lens camera module. Now, have a user's hand gently pick up the phone, naturally rotating it from the back to the front and bringing it upward toward their face. Clearly show the phone smoothly and quickly unlocking via Face ID recognition, transitioning immediately to a vibrant home screen filled with updated app icons. Finish the scene by subtly fading the home screen into the iconic Apple logo. Keep the visual style consistent, premium, and elegant, suitable for an official Apple product launch.\n",
        "</prompt>\n",
        "\n",
        "--\n",
        "\n",
        "Notice how we broke up the initial prompt into multiple prompts that provide context and continuity so this all works seamlessly.\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "2Q4VI67aDaJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Planner: ask the LLM to generate prompts (JSON)"
      ],
      "metadata": {
        "id": "c8KOZMx1Ts6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plan_prompts_with_ai(base_prompt: str, seconds_per_segment: int, num_generations: int):\n",
        "    \"\"\"\n",
        "    Calls the Responses API to produce a JSON object:\n",
        "    {\n",
        "      \"segments\": [\n",
        "        {\"title\": \"...\", \"seconds\": <int>, \"prompt\": \"<full prompt block>\"},\n",
        "        ...\n",
        "      ]\n",
        "    }\n",
        "    \"\"\"\n",
        "    # Compose a single plain-text input with the variables:\n",
        "    user_input = f\"\"\"\n",
        "BASE PROMPT: {base_prompt}\n",
        "\n",
        "GENERATION LENGTH (seconds): {seconds_per_segment}\n",
        "TOTAL GENERATIONS: {num_generations}\n",
        "\n",
        "Return exactly {num_generations} segments.\n",
        "\"\"\".strip()\n",
        "\n",
        "    # Minimal Responses API call; see docs & library readme for details.\n",
        "    # (If your account lacks the requested model, change PLANNER_MODEL accordingly.)\n",
        "    resp = client.responses.create(\n",
        "        model=PLANNER_MODEL,\n",
        "        instructions=PLANNER_SYSTEM_INSTRUCTIONS,\n",
        "        input=user_input,\n",
        "    )\n",
        "\n",
        "    text = getattr(resp, \"output_text\", None) or \"\"\n",
        "    if not text:\n",
        "        # Fallback: collect from structured blocks if needed\n",
        "        # (Different SDK versions may put text in resp.output or in content items.)\n",
        "        try:\n",
        "            # Attempt to reconstruct from generic fields\n",
        "            text = json.dumps(resp.to_dict())\n",
        "        except Exception:\n",
        "            raise RuntimeError(\"Planner returned no text; try changing PLANNER_MODEL.\")\n",
        "\n",
        "    # Extract the first JSON object found in the response text.\n",
        "    m = re.search(r'\\{[\\s\\S]*\\}', text)\n",
        "    if not m:\n",
        "        raise ValueError(\"Planner did not return JSON. Inspect response and adjust instructions.\")\n",
        "    data = json.loads(m.group(0))\n",
        "\n",
        "    # Basic validation and enforcement\n",
        "    segments = data.get(\"segments\", [])\n",
        "    if len(segments) != num_generations:\n",
        "        segments = segments[:num_generations]\n",
        "        # or pad/adjust; here we simply clamp.\n",
        "\n",
        "    # Force durations to the requested number (some models might deviate)\n",
        "    for seg in segments:\n",
        "        seg[\"seconds\"] = int(seconds_per_segment)\n",
        "\n",
        "    return segments\n",
        "\n",
        "segments_plan = plan_prompts_with_ai(BASE_PROMPT, SECONDS_PER_SEGMENT, NUM_GENERATIONS)\n",
        "\n",
        "print(\"AI‑planned segments:\\n\")\n",
        "for i, seg in enumerate(segments_plan, start=1):\n",
        "    print(f\"[{i:02d}] {seg['seconds']}s — {seg.get('title','(untitled)')}\")\n",
        "    print(seg[\"prompt\"])\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "9vtc8oHzSJiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Sora helpers (create → poll → download → extract final frame)"
      ],
      "metadata": {
        "id": "HWhkKTYDTp0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, mimetypes\n",
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "API_BASE = \"https://api.openai.com/v1\"\n",
        "HEADERS_AUTH = {\"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\"}\n",
        "\n",
        "def guess_mime(path: Path) -> str:\n",
        "    t = mimetypes.guess_type(str(path))[0]\n",
        "    return t or \"application/octet-stream\"\n",
        "\n",
        "def _dump_error(resp: requests.Response):\n",
        "    rid = resp.headers.get(\"x-request-id\", \"<none>\")\n",
        "    try:\n",
        "        body = resp.json()\n",
        "    except Exception:\n",
        "        body = resp.text\n",
        "    return f\"HTTP {resp.status_code} (request-id: {rid})\\n{body}\"\n",
        "\n",
        "def create_video(prompt: str, size: str, seconds: int, model: str, input_reference: Path | None):\n",
        "    \"\"\"\n",
        "    Always send multipart/form-data. This tends to be the most compatible with /videos,\n",
        "    and also supports input_reference seamlessly.\n",
        "    \"\"\"\n",
        "    files = {\n",
        "        \"model\":   (None, model),\n",
        "        \"prompt\":  (None, prompt),\n",
        "        \"seconds\": (None, str(seconds)),\n",
        "    }\n",
        "    if size:\n",
        "        files[\"size\"] = (None, size)\n",
        "\n",
        "    if input_reference is not None:\n",
        "        mime = guess_mime(input_reference)\n",
        "        files[\"input_reference\"] = (Path(input_reference).name, open(input_reference, \"rb\"), mime)\n",
        "\n",
        "    r = requests.post(f\"{API_BASE}/videos\", headers=HEADERS_AUTH, files=files, timeout=300)\n",
        "    if r.status_code >= 400:\n",
        "        raise RuntimeError(\"Create video failed:\\n\" + _dump_error(r))\n",
        "    return r.json()\n",
        "\n",
        "def retrieve_video(video_id: str):\n",
        "    r = requests.get(f\"{API_BASE}/videos/{video_id}\", headers=HEADERS_AUTH, timeout=60)\n",
        "    if r.status_code >= 400:\n",
        "        raise RuntimeError(\"Retrieve video failed:\\n\" + _dump_error(r))\n",
        "    return r.json()\n",
        "\n",
        "def download_video_content(video_id: str, out_path: Path, variant: str = \"video\"):\n",
        "    with requests.get(\n",
        "        f\"{API_BASE}/videos/{video_id}/content\",\n",
        "        headers=HEADERS_AUTH,\n",
        "        params={\"variant\": variant},\n",
        "        stream=True,\n",
        "        timeout=600,\n",
        "    ) as r:\n",
        "        if r.status_code >= 400:\n",
        "            raise RuntimeError(\"Download failed:\\n\" + _dump_error(r))\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                if chunk: f.write(chunk)\n",
        "    return out_path\n",
        "\n",
        "\n",
        "\n",
        "def poll_until_complete(job: dict, poll_interval=POLL_INTERVAL_SEC):\n",
        "    video = job\n",
        "    vid = video[\"id\"]\n",
        "\n",
        "    def bar(pct: float, width=30):\n",
        "        filled = int(max(0.0, min(100.0, pct)) / 100 * width)\n",
        "        return \"=\" * filled + \"-\" * (width - filled)\n",
        "\n",
        "    while video.get(\"status\") in (\"queued\", \"in_progress\"):\n",
        "        if PRINT_PROGRESS_BAR:\n",
        "            pct = float(video.get(\"progress\", 0) or 0)\n",
        "            status_text = \"Queued\" if video[\"status\"] == \"queued\" else \"Processing\"\n",
        "            print(f\"\\r{status_text}: [{bar(pct)}] {pct:5.1f}%\", end=\"\")\n",
        "        time.sleep(poll_interval)\n",
        "        video = retrieve_video(vid)\n",
        "\n",
        "    if PRINT_PROGRESS_BAR:\n",
        "        print()\n",
        "\n",
        "    if video.get(\"status\") != \"completed\":\n",
        "        msg = (video.get(\"error\") or {}).get(\"message\", f\"Job {vid} failed\")\n",
        "        raise RuntimeError(msg)\n",
        "    return video\n",
        "\n",
        "\n",
        "def extract_last_frame(video_path: Path, out_image_path: Path) -> Path:\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Failed to open {video_path}\")\n",
        "\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
        "    success, frame = False, None\n",
        "\n",
        "    if total > 0:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, total - 1)\n",
        "        success, frame = cap.read()\n",
        "    if not success or frame is None:\n",
        "        cap.release()\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        while True:\n",
        "            ret, f = cap.read()\n",
        "            if not ret: break\n",
        "            frame = f\n",
        "            success = True\n",
        "    cap.release()\n",
        "\n",
        "    if not success or frame is None:\n",
        "        raise RuntimeError(f\"Could not read last frame from {video_path}\")\n",
        "\n",
        "    out_image_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    ok = cv2.imwrite(str(out_image_path), frame)\n",
        "    if not ok:\n",
        "        raise RuntimeError(f\"Failed to write {out_image_path}\")\n",
        "    return out_image_path\n"
      ],
      "metadata": {
        "id": "jVZzK-vgSOMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Chain generator (use planner output; continuity via final frame)"
      ],
      "metadata": {
        "id": "K_DfcHGPTmxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chain_generate_sora(segments, size: str, model: str):\n",
        "    \"\"\"\n",
        "    segments: list of {\"title\": str, \"seconds\": int, \"prompt\": str}\n",
        "    Returns list of video segment Paths.\n",
        "    \"\"\"\n",
        "    input_ref = None\n",
        "    segment_paths = []\n",
        "\n",
        "    for i, seg in enumerate(segments, start=1):\n",
        "        secs   = int(seg[\"seconds\"])\n",
        "        prompt = seg[\"prompt\"]\n",
        "\n",
        "        print(f\"\\n=== Generating Segment {i}/{len(segments)} — {secs}s ===\")\n",
        "        job = create_video(prompt=prompt, size=size, seconds=secs, model=model, input_reference=input_ref)\n",
        "        print(\"Started job:\", job[\"id\"], \"| status:\", job[\"status\"])\n",
        "\n",
        "        completed = poll_until_complete(job)\n",
        "\n",
        "        seg_path = OUT_DIR / f\"segment_{i:02d}.mp4\"\n",
        "        download_video_content(completed[\"id\"], seg_path, variant=\"video\")\n",
        "        print(\"Saved\", seg_path)\n",
        "        segment_paths.append(seg_path)\n",
        "\n",
        "        # Prepare input reference (final frame) for the next segment\n",
        "        frame_path = OUT_DIR / f\"segment_{i:02d}_last.jpg\"\n",
        "        extract_last_frame(seg_path, frame_path)\n",
        "        print(\"Extracted last frame ->\", frame_path)\n",
        "        input_ref = frame_path\n",
        "\n",
        "    return segment_paths\n",
        "\n",
        "\n",
        "def concatenate_segments(segment_paths, out_path: Path) -> Path:\n",
        "    clips = [VideoFileClip(str(p)) for p in segment_paths]\n",
        "    target_fps = clips[0].fps or 24\n",
        "    result = concatenate_videoclips(clips, method=\"compose\")\n",
        "    result.write_videofile(\n",
        "        str(out_path),\n",
        "        codec=\"libx264\",\n",
        "        audio_codec=\"aac\",\n",
        "        fps=target_fps,\n",
        "        preset=\"medium\",\n",
        "        threads=0\n",
        "    )\n",
        "    for c in clips:\n",
        "        c.close()\n",
        "    return out_path\n"
      ],
      "metadata": {
        "id": "VzAQxmwwTPhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Run the whole pipeline"
      ],
      "metadata": {
        "id": "G16vwi3MTj5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) (Already ran) Plan prompts with AI -> segments_plan\n",
        "# 2) Generate with Sora 2 in a chain\n",
        "segment_paths = chain_generate_sora(segments_plan, size=SIZE, model=SORA_MODEL)\n",
        "\n",
        "# 3) Concatenate\n",
        "final_path = OUT_DIR / \"combined.mp4\"\n",
        "concatenate_segments(segment_paths, final_path)\n",
        "print(\"\\nWrote combined video:\", final_path)\n",
        "\n",
        "# 4) Inline preview\n",
        "display(IPyVideo(str(final_path), embed=True, width=768))\n"
      ],
      "metadata": {
        "id": "VUcGUc_ITSA3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}